**ReLU (Rectified Linear Unit)** is an activation function used in neural networks defined as:

ReLU(x)=max(0,x)

Where xxx is the input to the neuron.