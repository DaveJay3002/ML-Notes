**Convergence in Gradient Descent** refers to the process by which the gradient descent algorithm iteratively approaches the minimum of a cost (loss) function. Achieving convergence means that the algorithm has found parameter values (e.g., weights in a neural network) that minimize the cost function to an acceptable degree, resulting in an optimized model.

![[Pasted image 20241004233457.png]]

![[Pasted image 20241004233709.png]]

- Trying out different gradient descents for each learning rate gives us the most optimal learning rate.
- Can start be selecting a learning rate that is 0.0001 then making it go three times to 0.0003 and then 3 times to 0.001 and so on till 1 and then we can find which learning rate gives us the best cost function.