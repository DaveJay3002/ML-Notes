The **ADAM (Adaptive Moment Estimation)** algorithm is an optimization algorithm used in training deep learning models. It combines the advantages of two other popular optimization methods: **AdaGrad** (which adapts the learning rate based on the frequency of parameter updates) and **RMSProp** (which uses an exponentially decaying average of squared [[Gradient]]).
This used as an extension of [[Gradient Descent]] where ADAM algorithm adjusts the learning rate.
Always use ADAM Algorithm over plain gradient descent.
