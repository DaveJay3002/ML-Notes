**Gradient Descent** is an optimization algorithm commonly used in machine learning and deep learning to minimize the cost function of a model. The main idea behind gradient descent is to iteratively adjust the parameters of the model in order to reduce the error between the predicted and actual values.
![[Pasted image 20241004153436.png]]
# Outline
- Start with some parameters
- Keep changing the parameters to reduce J(w,b)
- Until we settle at or near a minimum
- Iterate with a new set of starting parameters so that we can find the global minima amongst local minimas
# Parameters
- [[Cost Function]]
- [[Gradient]]
- [[Learning Rate]]

# Intuition
![[Pasted image 20241004154227.png]]

# Formula 
![[Pasted image 20241004154830.png]]

- [[Convergence in Gradient Descent]]
- [[Gradient Descent for Logistic Regression]]